{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora Question Deduplication\n",
    "\n",
    "Experiments on the question deduplication dataset [provided by Quora](https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs). Each point in this dataset consists of two question titles and is labeled according to whether or not the titles correspond to duplicate questions.\n",
    "\n",
    "Currently, we use a unigram-bigram bag-of-words representation for each title with a choice of PCA or an two-layer autoencoder to reduce the unwieldy dimensionality of BoW vectors. We then use Keras to define and train a deep neural network to classify pairs of titles as duplicate or non-duplicate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cPickle as pickle\n",
    "import heapq\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rawDataGen(fname):\n",
    "    \"\"\"Generator over rows from the raw Quora dataset file. Yields dictionaries with keys\n",
    "    defined by the headers in the first row.\"\"\"\n",
    "    with open(fname) as f:\n",
    "        fields = f.readline().strip().split(\"\\t\")\n",
    "        line = f.readline()\n",
    "        while line != \"\":\n",
    "            yield {field:attr for field, attr in zip(fields, line.strip().split(\"\\t\"))}\n",
    "            line = f.readline()\n",
    "\n",
    "def preprocessText(text):\n",
    "    \"\"\"Currently converts to lower case, converts numbers to \"#\", separates out punctuation,\n",
    "    consolidates multiple whitespaces to one, and splits into tokens.\"\"\"\n",
    "    out = text.lower()\n",
    "    out = re.sub(r\"\\d+\", \"#\", out)\n",
    "    out = re.sub(r\"([(\\.\\.\\.)\\.!?:;,])\", r\" \\1 \", out)\n",
    "    out = re.sub(r\"\\s+\", \" \", out)\n",
    "    return out.split()\n",
    "\n",
    "def tokensToBow(tokens, vocab):\n",
    "    \"\"\"Given a unigram/bigram vocabulary and a list of tokens, generates a list\n",
    "    representing a bag-of-words featurization of the tokens.\"\"\"\n",
    "    bow = [0 for _ in range(len(vocab))]\n",
    "    for tok in tokens:\n",
    "        unigram = tuple([tok])\n",
    "        if unigram in vocab.keys():\n",
    "            bow[vocab[unigram]] += 1\n",
    "    for i in range(len(tokens) - 1):\n",
    "        bigram = tuple(tokens[i:i+2])\n",
    "        if bigram in vocab.keys():\n",
    "            bow[vocab[bigram]] += 1\n",
    "    return bow\n",
    "\n",
    "def analyzeDataset(dataFname):\n",
    "    \"\"\"Preprocesses the questions in each row of the dataset and builds\n",
    "    a word count based on the resulting token sequences.\"\"\"\n",
    "    wordCounts = defaultdict(int)\n",
    "    outputDocs = list()\n",
    "    numProcessed = 0\n",
    "    for doc in rawDataGen(dataFname):\n",
    "        try:\n",
    "            q1Processed = preprocessText(doc[\"question1\"])\n",
    "            q2Processed = preprocessText(doc[\"question2\"])\n",
    "            for nGramSize in (1, 2):\n",
    "                for tokens in (q1Processed, q2Processed):\n",
    "                    for idx in range(len(tokens) - (nGramSize - 1)):\n",
    "                        nGram = tuple(tokens[idx:idx + nGramSize])\n",
    "                        wordCounts[nGram] += 1\n",
    "            outputDocs.append({\n",
    "                \"q1\": q1Processed,\n",
    "                \"q2\": q2Processed,\n",
    "                \"label\": doc[\"is_duplicate\"]\n",
    "            })\n",
    "            numProcessed += 1\n",
    "            if numProcessed % 1000 == 0:\n",
    "                sys.stdout.write(\"\\r%d rows analyzed.\" % (numProcessed))\n",
    "                sys.stdout.flush()\n",
    "        except:\n",
    "            pass\n",
    "    sys.stdout.write(\"\\n\")\n",
    "    return outputDocs, wordCounts\n",
    "\n",
    "def generateVocab(counts, vocabSize=2000):\n",
    "    \"\"\"Given a dictionary of word/ngram counts, generates a vocabulary consisting of\n",
    "    the top vocabSize words.\"\"\"\n",
    "    sys.stdout.write(\"Generating vocab... \")\n",
    "    sys.stdout.flush()\n",
    "    countPairs = list(wordCounts.iteritems())\n",
    "    countPairs = [(term, count) for term, count in countPairs if count > 5]\n",
    "    topN = heapq.nlargest(vocabSize, countPairs, key=lambda pair: pair[1])\n",
    "    vocab = {term:idx for idx, (term, count) in enumerate(topN)}\n",
    "    sys.stdout.write(\"done!\\n\")\n",
    "    return vocab\n",
    "\n",
    "def finalizeDataset(analyzedDocs, vocab, maxRows=100000):\n",
    "    \"\"\"Builds a bag-of-words featurization for both Q1 and Q2 datasets as well\n",
    "    as an array of labels and returns all three.\"\"\"\n",
    "    q1List, q2List, labelsList = list(), list(), list()\n",
    "    numProcessed = 0\n",
    "    for doc in analyzedDocs[:maxRows]:\n",
    "        q1List.append(tokensToBow(doc[\"q1\"], vocab))\n",
    "        q2List.append(tokensToBow(doc[\"q2\"], vocab))\n",
    "        labelsList.append(int(doc[\"label\"]))\n",
    "        numProcessed += 1\n",
    "        if numProcessed % 100 == 0:\n",
    "            sys.stdout.write(\"\\r%d docs processed.\" % (numProcessed))\n",
    "            sys.stdout.flush()\n",
    "    sys.stdout.write(\"\\n\")\n",
    "    return np.array(q1List), np.array(q2List), np.array(labelsList)\n",
    "\n",
    "def saveData(dataSpec, q1, q2, labels, vocab):\n",
    "    \"\"\"Save the Q1/Q2/labels arrays and the vocabulary to files. Objects are\n",
    "    saved to `<dataSpec>_data.npz` and `<dataSpec>_vocab.pkl`.\"\"\"\n",
    "    with open(dataSpec + \"_data.npz\", \"w\") as dataF:\n",
    "        data = np.savez(dataF,\n",
    "                        q1=q1,\n",
    "                        q2=q2,\n",
    "                        labels=labels\n",
    "                        )\n",
    "    with open(dataSpec + \"_vocab.pkl\", \"w\") as vocabF:\n",
    "        pickle.dump(vocab, vocabF)\n",
    "\n",
    "def loadData(dataSpec):\n",
    "    \"\"\"Reads from the files saved by saveData.\"\"\"\n",
    "    with open(dataSpec + \"_data.npz\") as dataF:\n",
    "        data = np.load(dataF)\n",
    "        q1 = data[\"q1\"]\n",
    "        q2 = data[\"q2\"]\n",
    "        labels = data[\"labels\"]\n",
    "    with open(dataSpec + \"_vocab.pkl\") as vocabF:\n",
    "        vocab = pickle.load(vocabF)\n",
    "    return q1, q2, labels, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prep\n",
    "We use a simple unigram-bigram bag-of-words representation for now. More complex featurizations would include using an LSTM encoder, either on one-hot vectors or on learned vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "analyzedDocs, wordCounts = analyzeDataset(\"./quora_duplicate_questions.tsv\")\n",
    "vocab = generateVocab(wordCounts, vocabSize=5000)\n",
    "q1, q2, labels = finalizeDataset(analyzedDocs, vocab, maxRows=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saveData(\"unigram_bigram_5k\", q1, q2, labels, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q1, q2, labels, vocab = loadData(\"unigram_bigram_5k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction\n",
    "First, we try PCA to 1000 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allData = np.concatenate((q1[:10000, :], q2[:10000, :]))\n",
    "pca = PCA(n_components=1000)\n",
    "pca.fit(allData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"pca.pkl\", \"w\") as pcaF:\n",
    "    pickle.dump(pca, pcaF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"pca.pkl\") as pcaF:\n",
    "    pca = pickle.load(pcaF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reduce to to 1000 dimensions using PCA.\n",
    "nSmall = 200000\n",
    "q1Small = pca.transform(q1[:nSmall,:])\n",
    "q2Small = pca.transform(q2[:nSmall,:])\n",
    "labelsSmall = labels[:nSmall]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also experiment briefly with autoencoders. This is a simple two-layer fully-connected autoencoder to 1000 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180000 samples, validate on 20000 samples\n",
      "Epoch 1/1\n",
      "180000/180000 [==============================] - 556s - loss: 0.0042 - val_loss: 0.0013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x89fd35ad0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "\n",
    "autoencData = np.concatenate((q1[:100000], q2[:100000]))\n",
    "\n",
    "autoenc = Sequential([\n",
    "        Dense(2000, input_shape=(autoencData.shape[1],)),\n",
    "        BatchNormalization(),\n",
    "        Dense(1000),\n",
    "        BatchNormalization(),\n",
    "        Dense(2000),\n",
    "        Dense(5000)\n",
    "    ])\n",
    "\n",
    "autoenc.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "autoenc.fit(autoencData, autoencData, validation_split=0.1, nb_epoch=1, batch_size=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 2.          1.99352646]]\n",
      "\n",
      " [[ 1.          0.99872744]]\n",
      "\n",
      " [[ 1.          0.94655061]]\n",
      "\n",
      " ..., \n",
      " [[ 0.          0.01668848]]\n",
      "\n",
      " [[ 0.         -0.03442896]]\n",
      "\n",
      " [[ 0.         -0.01198013]]]\n"
     ]
    }
   ],
   "source": [
    "# Example from the autoencoder\n",
    "print np.array([autoencData[100:101], autoenc.predict(autoencData[100:101])]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Building the encoder using the weights from the trained autoencoder\n",
    "enc = Sequential([\n",
    "        Dense(2000, input_shape=(5000,)),\n",
    "        BatchNormalization(),\n",
    "        Dense(1000),\n",
    "    ])\n",
    "enc.set_weights(autoenc.get_weights()[:8])\n",
    "q1Small = enc.predict(q1)\n",
    "q2Small = enc.predict(q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "Now, we try the duplicate/non-duplicate classification task using the dimensionality-reduced data. We use a 4-layer fully-connected neural network with ReLU activation on the hidden layers and a softmax readout layer, as well as dropout and batch normalization layers to improve generalization and training time, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180000 samples, validate on 20000 samples\n",
      "Epoch 1/3\n",
      "180000/180000 [==============================] - 129s - loss: 0.5349 - acc: 0.7301 - fbeta_score: 0.7301 - val_loss: 0.4923 - val_acc: 0.7505 - val_fbeta_score: 0.7505\n",
      "Epoch 2/3\n",
      "180000/180000 [==============================] - 130s - loss: 0.4609 - acc: 0.7759 - fbeta_score: 0.7759 - val_loss: 0.4624 - val_acc: 0.7725 - val_fbeta_score: 0.7724\n",
      "Epoch 3/3\n",
      "180000/180000 [==============================] - 133s - loss: 0.4134 - acc: 0.8020 - fbeta_score: 0.8020 - val_loss: 0.4602 - val_acc: 0.7765 - val_fbeta_score: 0.7765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x8a1233850>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "clfDataSmall = np.concatenate((q1Small, q2Small), axis=1)\n",
    "\n",
    "model = Sequential([\n",
    "        Dense(2000, activation=\"relu\", input_shape=(clfDataSmall.shape[1],)),\n",
    "        Dropout(0.5),\n",
    "        Dense(1000, activation=\"relu\"),\n",
    "        Dense(200, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dense(2, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "labelsSmallOneHot = np.array([1 - labelsSmall, labelsSmall]).T\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(1e-2), metrics=[\"accuracy\", \"fbeta_score\"])\n",
    "model.fit(clfDataSmall, labelsSmallOneHot, validation_split=0.1, nb_epoch=3, batch_size=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For reference, we look at the distribution of labels in the dataset. We see below that 63% of pairs are non-duplicates and 37% are duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.62758  0.37242]\n"
     ]
    }
   ],
   "source": [
    "print labelsSmallOneHot.sum(0).astype(float) / labelsSmallOneHot.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
