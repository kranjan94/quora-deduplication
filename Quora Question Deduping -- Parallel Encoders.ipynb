{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora Duplicate Question Detection: Parallel Encoder Architecture\n",
    "This model simultaneously trains a distributed word embedding and an LSTM encoder for sequences from that embedding. It uses this embedding encoder to encode two Quora question titles into fixed-length vectors and then uses a two-layer neural network with batch normalization and dropout to classify the pair as duplicate or non-duplicate.\n",
    "\n",
    "Currently, this performs slightly better than the bag-of-words featurization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cPickle as pickle\n",
    "import heapq\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "SEQ_LEN = 20\n",
    "EMBED_DIM = 100\n",
    "ENCODE_DIM = 128\n",
    "DROPOUT_PARAM = 0.5\n",
    "H_DIM = 256\n",
    "\n",
    "# Training parameters\n",
    "\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rawDataGen(fname):\n",
    "    \"\"\"Generator over rows from the raw Quora dataset file. Yields dictionaries with keys\n",
    "    defined by the headers in the first row.\"\"\"\n",
    "    with open(fname) as f:\n",
    "        fields = f.readline().strip().split(\"\\t\")\n",
    "        line = f.readline()\n",
    "        while line != \"\":\n",
    "            yield {field:attr for field, attr in zip(fields, line.strip().split(\"\\t\"))}\n",
    "            line = f.readline()\n",
    "\n",
    "def preprocessText(text):\n",
    "    \"\"\"Currently converts to lower case, converts numbers to \"#\", separates out punctuation,\n",
    "    consolidates multiple whitespaces to one, and splits into tokens.\"\"\"\n",
    "    out = text.lower()\n",
    "    out = re.sub(r\"\\d+\", \"#\", out)\n",
    "    out = re.sub(r\"([(\\.\\.\\.)\\.!?:;,])\", r\" \\1 \", out)\n",
    "    out = re.sub(r\"\\s+\", \" \", out)\n",
    "    return out.split()\n",
    "\n",
    "def preprocessData(inFname, maxDocs=None, outFname=None):\n",
    "    \"\"\"Read (up to maxDocs) lines from the file at inFname, preprocess the questions,\n",
    "    and return (q1, q2, label) tuples in a list. Optionally save the list as a\n",
    "    pickle file.\"\"\"\n",
    "    preprocessed = list()\n",
    "    for doc in rawDataGen(inFname):\n",
    "        try:\n",
    "            q1 = preprocessText(doc[\"question1\"])\n",
    "            q2 = preprocessText(doc[\"question2\"])\n",
    "            label = int(doc[\"is_duplicate\"])\n",
    "            preprocessed.append((q1, q2, label))\n",
    "        except:  # Some corrupted lines in the input\n",
    "            pass\n",
    "        if len(preprocessed) % 1000 == 0:\n",
    "            sys.stdout.write(\"\\rPreprocessed %d documents.\" % len(preprocessed))\n",
    "            sys.stdout.flush()\n",
    "        if len(preprocessed) == maxDocs:\n",
    "            break\n",
    "    sys.stdout.write(\"\\n\")\n",
    "    \n",
    "    if outFname:\n",
    "        sys.stdout.write(\"Writing preprocessed data file... \")\n",
    "        sys.stdout.flush()\n",
    "        with open(outFname, \"w\") as outF:\n",
    "            pickle.dump(preprocessed, outF)\n",
    "        sys.stdout.write(\"done!\\n\")\n",
    "    return preprocessed\n",
    "\n",
    "def generateVocab(preprocessed, vocabSize=10000, vocabFname=None):\n",
    "    \"\"\"Read from the preprocessed questions and generate a vocab.\"\"\"\n",
    "    wordCounts = defaultdict(int)\n",
    "    sys.stdout.write(\"Performing word count... \")\n",
    "    sys.stdout.flush()\n",
    "    for q1, q2, _ in preprocessed:\n",
    "        for word in q1 + q2:\n",
    "            wordCounts[word] += 1\n",
    "    sys.stdout.write(\"done!\\n\")\n",
    "    \n",
    "    sys.stdout.write(\"Building vocab... \")\n",
    "    sys.stdout.flush()\n",
    "    wordCountsList = list(wordCounts.iteritems())\n",
    "    topNWordCounts = heapq.nlargest(vocabSize, wordCountsList, key=lambda wc: wc[1])\n",
    "    topNWords = [word for word, _ in topNWordCounts]\n",
    "    allVocabWords = [\"PAD\", \"UNK\"] + topNWords\n",
    "    vocab = {word:idx for idx, word in enumerate(allVocabWords)}\n",
    "    sys.stdout.write(\"done!\\n\")\n",
    "    \n",
    "    if vocabFname:\n",
    "        sys.stdout.write(\"Writing vocab file... \")\n",
    "        sys.stdout.flush()\n",
    "        with open(vocabFname, \"w\") as vocabF:\n",
    "            pickle.dump(vocab, vocabF)\n",
    "        sys.stdout.write(\"done!\\n\")\n",
    "    return vocab\n",
    "\n",
    "def indexSequence(tokens, vocab):\n",
    "    \"\"\"Given a list of tokens and a vocab, return a list of vocab indices corresponding\n",
    "    to the tokens.\"\"\"\n",
    "    return [vocab[tok] if tok in vocab.keys() else vocab[\"UNK\"] for tok in tokens]\n",
    "\n",
    "def indexDataset(preprocessed, vocab, indexedFname=None):\n",
    "    \"\"\"Indexes the questions in the entire dataset using the provided vocab. Optionally\n",
    "    writes the output to file.\"\"\"\n",
    "    indexed = list()\n",
    "    for q1, q2, label in preprocessed:\n",
    "        indexedSample = (indexSequence(q1, vocab), indexSequence(q2, vocab), label)\n",
    "        indexed.append(indexedSample)\n",
    "        if len(indexed) % 100 == 0:\n",
    "            sys.stdout.write(\"\\rIndexed %d documents.\" % len(indexed))\n",
    "            sys.stdout.flush()\n",
    "    sys.stdout.write(\"\\n\")\n",
    "    \n",
    "    if indexedFname:\n",
    "        sys.stdout.write(\"Writing indexed data file... \")\n",
    "        sys.stdout.flush()\n",
    "        with open(indexedFname, \"w\") as indexedF:\n",
    "            pickle.dump(indexed, indexedF)\n",
    "        sys.stdout.write(\"done!\\n\")\n",
    "    return indexed\n",
    "        \n",
    "def prepareArrays(indexed, seqLen, padValue, oneHotLabels=True, dataFname=None):\n",
    "    \"\"\"Align all questions to seqLen tokens using padValue to pad and truncating\n",
    "    where necessary and then return three numpy arrays: q1, q2, and labels.\n",
    "    Labels is one-hot by default, or a single vector of 0s and 1s if\n",
    "    oneHotLabels is False. Optionally writes the arrays to a numpy NPZ file.\"\"\"\n",
    "    q1List, q2List, labelsList = list(), list(), list()\n",
    "    for q1Indexed, q2Indexed, label in indexed:\n",
    "        q1Aligned = q1Indexed[:seqLen] + [padValue] * (seqLen - len(q1Indexed))\n",
    "        q2Aligned = q2Indexed[:seqLen] + [padValue] * (seqLen - len(q2Indexed))\n",
    "        q1List.append(q1Aligned)\n",
    "        q2List.append(q2Aligned)\n",
    "        labelsList.append([int(not label), int(label)] if oneHotLabels else label)\n",
    "        if len(q1List) % 1000 == 0:\n",
    "            sys.stdout.write(\"\\rPrepared %d documents for data arrays.\" % len(q1List))\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "    sys.stdout.write(\"\\nConstructing arrays... \")\n",
    "    sys.stdout.flush()\n",
    "    q1, q2, labels = np.array(q1List), np.array(q2List), np.array(labelsList)\n",
    "    sys.stdout.write(\"done! Q1: %s; Q2: %s; labels: %s\\n\" % (q1.shape, q2.shape, labels.shape))\n",
    "    \n",
    "    if dataFname:\n",
    "        sys.stdout.write(\"Writing final data file... \")\n",
    "        sys.stdout.flush()\n",
    "        with open(dataFname, \"w\") as dataF:\n",
    "            np.savez(dataF,\n",
    "                     q1=q1,\n",
    "                     q2=q2,\n",
    "                     labels=labels)\n",
    "        sys.stdout.write(\"done!\\n\")\n",
    "    return q1, q2, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed 404000 documents.\n",
      "Writing preprocessed data file... done!\n",
      "Performing word count... done!\n",
      "Building vocab... done!\n",
      "Writing vocab file... done!\n",
      "Indexed 404300 documents.\n",
      "Writing indexed data file... done!\n",
      "Prepared 404000 documents for data arrays.\n",
      "Constructing arrays... done! Q1: (404340, 20); Q2: (404340, 20); labels: (404340, 2)\n",
      "Writing final data file... done!\n"
     ]
    }
   ],
   "source": [
    "# Data pipeline\n",
    "\n",
    "preprocessed = preprocessData(\"quora_duplicate_questions.tsv\", outFname=\"data/preprocessed.pkl\")\n",
    "vocab = generateVocab(preprocessed, vocabFname=\"data/vocab.pkl\")\n",
    "indexed = indexDataset(preprocessed, vocab, indexedFname=\"data/indexed.pkl\")\n",
    "q1, q2, labels = prepareArrays(indexed, SEQ_LEN, vocab[\"PAD\"], dataFname=\"data/data.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The detailed architecture is as follows:\n",
    "\n",
    "* Inputs: two 20-length sequences of vocabulary indices\n",
    "* 200-dimension word embedding\n",
    "* Bidirectional LSTM-128 encoder\n",
    "* Concatenation of encoded vectors\n",
    "* Batch normalization\n",
    "* FC-256 + ReLU hidden layer\n",
    "* Dropout w/ probability 0.5\n",
    "* FC-2 + softmax readout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 384123 samples, validate on 20217 samples\n",
      "Epoch 1/3\n",
      "384123/384123 [==============================] - 1149s - loss: 0.4882 - acc: 0.7630 - val_loss: 0.4272 - val_acc: 0.7975\n",
      "Epoch 2/3\n",
      "384123/384123 [==============================] - 3756s - loss: 0.3963 - acc: 0.8158 - val_loss: 0.4015 - val_acc: 0.8113\n",
      "Epoch 3/3\n",
      "384123/384123 [==============================] - 910s - loss: 0.3470 - acc: 0.8431 - val_loss: 0.3994 - val_acc: 0.8208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19acb5550>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model definition and training\n",
    "# We use Keras' functional API to enable sharing of the embedding layer and BLSTM encoder between\n",
    "# the two questions.\n",
    "\n",
    "from keras.layers import Input, Embedding, Bidirectional, LSTM, merge, Dense, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "q1Input = Input(shape=(SEQ_LEN,))\n",
    "q2Input = Input(shape=(SEQ_LEN,))\n",
    "\n",
    "embed = Embedding(len(vocab), EMBED_DIM, input_length=SEQ_LEN)\n",
    "q1Embed = embed(q1Input)\n",
    "q2Embed = embed(q2Input)\n",
    "\n",
    "encoder = Bidirectional(LSTM(ENCODE_DIM))\n",
    "q1Encoded = encoder(q1Embed)\n",
    "q2Encoded = encoder(q2Embed)\n",
    "\n",
    "merged = merge([q1Encoded, q2Encoded], mode='concat', concat_axis=-1)\n",
    "mergedBN = BatchNormalization()(merged)\n",
    "\n",
    "h = Dense(H_DIM, activation=\"relu\")(mergedBN)\n",
    "hDrop = Dropout(DROPOUT_PARAM)(h)\n",
    "preds = Dense(2, activation=\"softmax\")(hDrop)\n",
    "\n",
    "model = Model(input=[q1Input, q2Input], output=preds)\n",
    "model.compile(optimizer=Adam(LR), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit([q1, q2], labels, nb_epoch=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
